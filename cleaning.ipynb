{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5996a5b1",
   "metadata": {},
   "source": [
    "# Data Cleaning Overview\n",
    "This notebook's purpose is to perform the dataset cleaning steps including those detailed in the Exploration notebook, but also any additional needed as further exploration is done.\n",
    "\n",
    "While the visualization of the data in this notebook is interesting to the analyzer, a production process for cleaning the data should live in a script instead. This notebook is showing a proof of concept of data cleaning.\n",
    "\n",
    "\n",
    "## Expected Cleaning Steps\n",
    "- Remove `url`, `email_link`, and `date_of_collection` columns\n",
    "- Separate `rfs` columns into granularity aspects (year, month/quarter) and cast to appropriate types\n",
    "- Due to many to many relationship, separate owners and landing_point information into separate tablelike datasets.\n",
    "- Cast `length` to numeric and add unit information to column name\n",
    "- Fill `is_tbd` nulls with False\n",
    "- Collect and append latitude/longitude data for landing points\n",
    "- Potentially feature engineer to get landing point city\n",
    "\n",
    "## Actual Cleaning Steps\n",
    "- Remove `url`, `email_link`, and `date_of_collection` columns\n",
    "- Cast `length` to numeric and add unit information to column name\n",
    "- Separate `rfs` columns into granularity aspects (year, month/quarter) and cast to appropriate types\n",
    "- Drop `rfs_sub_year` columns due to the significant number of null values. In the future, may still be useful\n",
    "- Due to many to many relationship, separate owners and landing_point information into separate tablelike datasets.\n",
    "- feature engineer to get landing point sub country granularity from `name` field by removing country info\n",
    "- Fill `is_tbd` nulls with False\n",
    "- Collect and create separate latitude/longitude data for landing points\n",
    "    - Ended up making it a separate dataset to cut down on data duplication\n",
    "    - Required several iterations and extensive exploration. See below for further information.\n",
    "- Save datasets locally and then upload to S3\n",
    "    - Done because this is a one-time thing. If process runs multiple times, should code out programatic upload.\n",
    "\n",
    "## Lat/Long Coordinate Append Steps\n",
    "In order to visualize the location of landing points on a map, coordinates in latitude and longitude are required but are not provided by the dataset. To collect this information, I attempted several different methods. In the end, a mixture of methods was used.\n",
    "\n",
    "For landing site granularity, so that different landing points within the same country are separated visually, I cleaned the `name` field to attempt to pull out sub country information including city and region. However, this level of specificity proved difficult to collect accurate coordinates for due to API limits and missing address information (ie, San Jose in Phillipines. There are 9 different cities in the phillipines with that city name.) So, along with more specific coordinates, I also included overall country coordinates in the final output.\n",
    "\n",
    "Data Issues:\n",
    "- Country name not consistent across datasets, required cleaning\n",
    "- City name not consistent across datasets (punctuation), was not cleaned due to sheer number\n",
    "\n",
    "Methods considered:\n",
    "1. Find public dataset of city coordinates\n",
    "    - I used https://simplemaps.com/data/world-cities, which is free-use so long as credit is given.\n",
    "    - Also explored https://www.geonames.org/maps/addresses.html#geoCodeAddress, but this is missing coordinates for most countries.\n",
    "    - Issues:\n",
    "        - Not all cities are listed resulted in many missing datapoints\n",
    "        - Only half of landing point locations are able to be matched using this dataset\n",
    "    - Next Steps Options:\n",
    "        - Use Levenstein distance to calculate a fuzzy match on city. Did not pursue now because this could easily match to incorrect city if off by a single letter.\n",
    "2. Use a free API to pass address info and get back coordinates.\n",
    "    - Because the dataset is so small, maybe we can use an API to collect info without hitting rate limits\n",
    "    - Used https://www.gpsvisualizer.com/geocoder/ with MapQuest application token.\n",
    "    - Issues:\n",
    "        - Slow, returned one result every second\n",
    "        - Doesn't seem to be able to handle city, country combos. Had lots of incorrect mappings (ie, China city mapped to Washington DC USA coordinates).\n",
    "3. Find a public dataset of overall country coordinates.\n",
    "    - Abandoning granularity, at least append the overall country coordinates.\n",
    "    - Here, I used the Kaggle dataset https://www.kaggle.com/datasets/paultimothymooney/latitude-and-longitude-for-every-country-and-state which also included US state coordinates.\n",
    "    - Issues:\n",
    "        - Missing a couple countries in the submarine dataset. Needed to be appended using hardcoding.\n",
    "\n",
    "Approach: \n",
    "- Have two columns that have the most granularity available (`latitude` and `longitude`) and two columns that simply have the overall country coordinates (`latitude_country` and `longitude_country`).\n",
    "- The lowest granularity for a landing point was found by passing the location data through several matching processes that became less and less granular. They are as follows:\n",
    "    - Match Level 1: City, Region, and Country\n",
    "    - Match Level 2: City and Country\n",
    "    - Match Level 3 (USA Only): USA State and Country\n",
    "    - Match Level 4: Country\n",
    "    - Match Level 5: Hardcoded Country\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7574cc2d",
   "metadata": {},
   "source": [
    "# Imports and Raw Cable Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03dde3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from us_state_abbr import abbrev_to_us_state\n",
    "\n",
    "# Constants\n",
    "RAW_DATA_PATH = Path(\"raw_data\")\n",
    "CLEAN_DATA_PATH = Path(\"cleaned_data\")\n",
    "\n",
    "RAW_CABLE_DATA_PATH = RAW_DATA_PATH / \"Submarine Cables - 2023-02-22.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5f94a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sub cable datasets\n",
    "with RAW_CABLE_DATA_PATH.open(\"r\") as data_file:\n",
    "    raw_cable_data = json.load(data_file)\n",
    "\n",
    "# Switch from json to dataframe for easier manipulation\n",
    "raw_cable_df = pd.DataFrame.from_dict(raw_cable_data)\n",
    "\n",
    "# Create new object to represent manipulated data\n",
    "clean_cable_df = raw_cable_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb7b41c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cable_id</th>\n",
       "      <th>cable_name</th>\n",
       "      <th>owners</th>\n",
       "      <th>landing_points</th>\n",
       "      <th>length (km)</th>\n",
       "      <th>rfs_year</th>\n",
       "      <th>rfs_sub_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2africa</td>\n",
       "      <td>2Africa</td>\n",
       "      <td>China Mobile, MTN, Meta, Orange, Saudi Telecom...</td>\n",
       "      <td>[{\"is_tbd\": null, \"country\": \"Angola\", \"id\": \"...</td>\n",
       "      <td>45000</td>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acs-alaska-oregon-network-akorn</td>\n",
       "      <td>ACS Alaska-Oregon Network (AKORN)</td>\n",
       "      <td>Alaska Communications</td>\n",
       "      <td>[{\"is_tbd\": null, \"country\": \"United States\", ...</td>\n",
       "      <td>3000</td>\n",
       "      <td>2009</td>\n",
       "      <td>April</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aden-djibouti</td>\n",
       "      <td>Aden-Djibouti</td>\n",
       "      <td>Djibouti Telecom, Orange, Tata Communications,...</td>\n",
       "      <td>[{\"is_tbd\": null, \"country\": \"Djibouti\", \"id\":...</td>\n",
       "      <td>269</td>\n",
       "      <td>1994</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adria-1</td>\n",
       "      <td>Adria-1</td>\n",
       "      <td>ALBtelecom, Hrvatski Telekom</td>\n",
       "      <td>[{\"is_tbd\": null, \"country\": \"Albania\", \"id\": ...</td>\n",
       "      <td>440</td>\n",
       "      <td>1996</td>\n",
       "      <td>September</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aec-1</td>\n",
       "      <td>AEC-1</td>\n",
       "      <td>Aqua Comms</td>\n",
       "      <td>[{\"is_tbd\": null, \"country\": \"Ireland\", \"id\": ...</td>\n",
       "      <td>5521</td>\n",
       "      <td>2016</td>\n",
       "      <td>January</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          cable_id                         cable_name  \\\n",
       "0                          2africa                            2Africa   \n",
       "1  acs-alaska-oregon-network-akorn  ACS Alaska-Oregon Network (AKORN)   \n",
       "2                    aden-djibouti                      Aden-Djibouti   \n",
       "3                          adria-1                            Adria-1   \n",
       "4                            aec-1                              AEC-1   \n",
       "\n",
       "                                              owners  \\\n",
       "0  China Mobile, MTN, Meta, Orange, Saudi Telecom...   \n",
       "1                              Alaska Communications   \n",
       "2  Djibouti Telecom, Orange, Tata Communications,...   \n",
       "3                       ALBtelecom, Hrvatski Telekom   \n",
       "4                                         Aqua Comms   \n",
       "\n",
       "                                      landing_points  length (km)  rfs_year  \\\n",
       "0  [{\"is_tbd\": null, \"country\": \"Angola\", \"id\": \"...        45000      2023   \n",
       "1  [{\"is_tbd\": null, \"country\": \"United States\", ...         3000      2009   \n",
       "2  [{\"is_tbd\": null, \"country\": \"Djibouti\", \"id\":...          269      1994   \n",
       "3  [{\"is_tbd\": null, \"country\": \"Albania\", \"id\": ...          440      1996   \n",
       "4  [{\"is_tbd\": null, \"country\": \"Ireland\", \"id\": ...         5521      2016   \n",
       "\n",
       "  rfs_sub_year  \n",
       "0          NaN  \n",
       "1        April  \n",
       "2          NaN  \n",
       "3    September  \n",
       "4      January  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CLEANING: Remove `url`, `email_link`, and `date_of_collection` columns\n",
    "clean_cable_df = clean_cable_df.drop(columns=['date_of_collection', 'url', 'email_link'])\n",
    "\n",
    "\n",
    "# CLEANING: Cast `length` to numeric and add unit information to column name\n",
    "clean_cable_df['length'] = clean_cable_df['length'].str[:-3].str.replace(',', '').astype('Int64')\n",
    "clean_cable_df = clean_cable_df.rename(columns={'length': 'length (km)'})\n",
    "\n",
    "\n",
    "# CLEANING: Separate `rfs` columns into granularity aspects (year, month/quarter) and cast to appropriate types\n",
    "#     Remove raw rfs column\n",
    "rfs_split = clean_cable_df['rfs'].str.split()\n",
    "assert rfs_split.str.len().max() <= 2 # make sure there are at max two data points\n",
    "clean_cable_df['rfs_year'] = rfs_split.str[0].astype('Int16') # Int16 to handle nulls\n",
    "clean_cable_df['rfs_sub_year'] = rfs_split.str[1]\n",
    "clean_cable_df = clean_cable_df.drop(columns=['rfs'])\n",
    "\n",
    "clean_cable_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67920684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of the rfs year:\n",
      " count          540.0\n",
      "mean     2010.298148\n",
      "std        10.063343\n",
      "min           1989.0\n",
      "25%           2001.0\n",
      "50%           2011.0\n",
      "75%           2019.0\n",
      "max           2026.0\n",
      "Name: rfs_year, dtype: Float64\n",
      "\n",
      "Distribution of the length:\n",
      " count          508.0\n",
      "mean     3425.437008\n",
      "std      6001.366601\n",
      "min              5.0\n",
      "25%           228.75\n",
      "50%            816.0\n",
      "75%           3478.5\n",
      "max          45000.0\n",
      "Name: length (km), dtype: Float64\n",
      "\n",
      "Value counts of the rfs sub year unit:\n",
      " NaN          172\n",
      "December      47\n",
      "June          32\n",
      "September     29\n",
      "January       29\n",
      "March         29\n",
      "July          28\n",
      "August        24\n",
      "November      24\n",
      "October       23\n",
      "February      21\n",
      "April         20\n",
      "May           18\n",
      "Q1            14\n",
      "Q4            12\n",
      "Q3             9\n",
      "Q2             9\n",
      "None           5\n",
      "Name: rfs_sub_year, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Explore cleaned columns again\n",
    "\n",
    "print(\"Distribution of the rfs year:\\n\", clean_cable_df['rfs_year'].describe())\n",
    "\n",
    "print(\"\\nDistribution of the length:\\n\", clean_cable_df['length (km)'].describe())\n",
    "\n",
    "# Don't want to make the assumption that Q1 means beginning of year as they may be business quarters\n",
    "# FINDING: Given large number of nulls and mismatch of month vs quarter, dropping this col for now\n",
    "#     may still be of use when additional data can be provided\n",
    "print(\"\\nValue counts of the rfs sub year unit:\\n\", clean_cable_df['rfs_sub_year'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eed98d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEANING: Drop `rfs_sub_year` columns due to the significant number of null values. In the future, may still be useful\n",
    "clean_cable_df.drop(columns=['rfs_sub_year'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1ff481",
   "metadata": {},
   "source": [
    "# Split into three datasets\n",
    "Cable, Owner, Landing Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ec1b222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because cable_id is unique, there will be no duplicate rows in the below datasets\n",
    "cable_data = clean_cable_df[[\"cable_id\", \"cable_name\", \"length (km)\", \"rfs_year\"]].copy()\n",
    "owner_data = clean_cable_df[['owners', 'cable_id']].copy()\n",
    "location_data = clean_cable_df[['landing_points', 'cable_id']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "78b493a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>owner</th>\n",
       "      <th>cable_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>China Mobile</td>\n",
       "      <td>2africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MTN</td>\n",
       "      <td>2africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Meta</td>\n",
       "      <td>2africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Orange</td>\n",
       "      <td>2africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Saudi Telecom</td>\n",
       "      <td>2africa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           owner cable_id\n",
       "0   China Mobile  2africa\n",
       "1            MTN  2africa\n",
       "2           Meta  2africa\n",
       "3         Orange  2africa\n",
       "4  Saudi Telecom  2africa"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "owner_data['owners'] = owner_data['owners'].str.split(', ')\n",
    "owner_data = owner_data.explode('owners', ignore_index=True).rename(columns={'owners': 'owner'})\n",
    "owner_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d75762ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2578 entries, 0 to 2577\n",
      "Data columns (total 6 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   is_tbd    2578 non-null   bool  \n",
      " 1   country   2578 non-null   object\n",
      " 2   id        2578 non-null   object\n",
      " 3   cable_id  2578 non-null   object\n",
      " 4   region    351 non-null    object\n",
      " 5   city      2578 non-null   object\n",
      "dtypes: bool(1), object(5)\n",
      "memory usage: 103.3+ KB\n",
      "Column types and nulls:\n",
      " None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_tbd</th>\n",
       "      <th>country</th>\n",
       "      <th>id</th>\n",
       "      <th>cable_id</th>\n",
       "      <th>region</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>Angola</td>\n",
       "      <td>luanda-angola</td>\n",
       "      <td>2africa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Luanda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>Bahrain</td>\n",
       "      <td>manama-bahrain</td>\n",
       "      <td>2africa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Manama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>Comoros</td>\n",
       "      <td>moroni-comoros</td>\n",
       "      <td>2africa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Moroni</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_tbd  country              id cable_id region    city\n",
       "0   False   Angola   luanda-angola  2africa    NaN  Luanda\n",
       "1   False  Bahrain  manama-bahrain  2africa    NaN  Manama\n",
       "2   False  Comoros  moroni-comoros  2africa    NaN  Moroni"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Countries are in alphabetical order, so probably not in cable order. Don't need to track order\n",
    "# lambda is slow here, but dataset is small. If dataset scales, will need a new solution\n",
    "location_data['landing_points'] = location_data['landing_points'].apply(lambda x: json.loads(x))\n",
    "location_data = location_data.explode('landing_points', ignore_index=True)\n",
    "location_data = pd.concat([location_data['landing_points'].apply(pd.Series), location_data['cable_id']], axis=1)\n",
    "\n",
    "# CLEANING: feature engineer to get landing point city and region granularity from `name` field \n",
    "#     by removing country info\n",
    "# Remove `name` column after as it contains no additional information\n",
    "location_data['sub_country'] = location_data.apply(lambda x: x['name'].replace(\", \" + x['country'], \"\"), axis=1)\n",
    "loc_split = location_data['sub_country'].str.split(\", \")\n",
    "location_data['region'] = loc_split.str[1]\n",
    "location_data['city'] = loc_split.str[0]\n",
    "location_data.drop(columns=['name', 'sub_country'], inplace=True)\n",
    "\n",
    "\n",
    "# CLEANING: Fill `is_tbd` nulls with False\n",
    "location_data['is_tbd'] = location_data['is_tbd'].fillna(False)\n",
    "\n",
    "\n",
    "# Visualize\n",
    "print(\"Column types and nulls:\\n\", location_data.info())\n",
    "location_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a12b8c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create latitude / longitude dataset\n",
    "\n",
    "# Saved dataset of world city latitude/longitude\n",
    "RAW_LAT_LONG_CITY_PATH = RAW_DATA_PATH / \"worldcities.csv\"\n",
    "RAW_LAT_LONG_COUNTRY_HARDCODED_PATH = RAW_DATA_PATH / \"country_lat_long_hard_coded.csv\"\n",
    "RAW_LAT_LONG_US_STATE_AND_COUNTRY_PATH = RAW_DATA_PATH / \"country_and_usa_states_lat_long.csv\"\n",
    "\n",
    "\n",
    "# Hard code some country name translations because no 2-letter country code is provided in submarine dataset\n",
    "country_cleaning = {\n",
    "    \"British Virgin Islands\": \"Virgin Islands (U.K.)\",\n",
    "    \"U.S. Virgin Islands\": \"Virgin Islands (U.S.)\",\n",
    "    \"Congo [DRC]\": \"Congo, Dem. Rep.\",\n",
    "    \"Congo [Republic]\": \"Congo, Rep.\",\n",
    "    \"Myanmar [Burma]\": \"Myanmar\",\n",
    "    \"Saint Helena\": \"Saint Helena, Ascension and Tristan da Cunha\",\n",
    "    \"Cocos [Keeling] Islands\": \"Cocos (Keeling) Islands\",\n",
    "}\n",
    "\n",
    "\n",
    "# Match 1 Dataset: Dataset for match city, admin, country\n",
    "lat_long_city_admin_country = (\n",
    "    pd.read_csv(RAW_LAT_LONG_CITY_PATH)[['city_ascii', 'admin_name', 'country', 'lat', 'lng']]\n",
    "    .rename(columns={'city_ascii': 'city', 'lat': 'latitude', 'lng': 'longitude', 'admin_name': 'region'})\n",
    ")\n",
    "lat_long_city_admin_country['country'] = (\n",
    "    lat_long_city_admin_country['country']\n",
    "    .apply(lambda x: country_cleaning.get(x, x))\n",
    ")\n",
    "\n",
    "\n",
    "# Match 2 Dataset: Dataset for match city, country\n",
    "lat_long_city_country = lat_long_city_admin_country.drop(columns=['region'])\n",
    "unique_combos = lat_long_city_country.groupby(['country', 'city']).nunique().reset_index()\n",
    "unique_combos = unique_combos[(unique_combos['latitude'] == 1) & (unique_combos['longitude'] == 1)]\n",
    "lat_long_city_country = (\n",
    "    lat_long_city_country\n",
    "    .merge(unique_combos[['country', 'city']], on=['country', 'city'], how='inner')\n",
    ")\n",
    "\n",
    "\n",
    "# Match 3 Dataset: Dataset for match usa state\n",
    "lat_long_country_state = pd.read_csv(RAW_LAT_LONG_US_STATE_AND_COUNTRY_PATH)\n",
    "lat_long_country_state['country'] = lat_long_country_state['country'].apply(lambda x: country_cleaning.get(x, x))\n",
    "\n",
    "lat_long_usa_state = (\n",
    "    lat_long_country_state[['usa_state', 'country', 'usa_state_latitude', 'usa_state_longitude']]\n",
    "    .copy()\n",
    "    .dropna()\n",
    "    .rename(columns={'usa_state_latitude': 'latitude', 'usa_state_longitude': 'longitude', 'usa_state': 'region'})\n",
    ")\n",
    "lat_long_usa_state['country'] = 'United States'\n",
    "\n",
    "\n",
    "# Match 4 Dataset: Dataset for match country\n",
    "lat_long_country = lat_long_country_state[['country', 'latitude', 'longitude']].copy()\n",
    "\n",
    "\n",
    "# Match 5 Dataset: Hardcoded for match country\n",
    "lat_long_country_hardcoded = pd.read_csv(RAW_LAT_LONG_COUNTRY_HARDCODED_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab34371d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "match 1 percentage: 0.0249, count: 39\n",
      "remaining percentage: 0.9751, count: 1525\n",
      "\n",
      "match 2 percentage: 0.3242, count: 507\n",
      "remaining percentage: 0.6509, count: 1018\n",
      "\n",
      "match 3 percentage: 0.0486, count: 76\n",
      "remaining percentage: 0.6023, count: 942\n",
      "\n",
      "match 4 percentage: 0.5953, count: 931\n",
      "remaining percentage: 0.0070, count: 11\n",
      "\n",
      "match 4 percentage: 0.0070, count: 11\n",
      "remaining percentage: 0.0000, count: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>country</th>\n",
       "      <th>city</th>\n",
       "      <th>region</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anchorage-ak-united-states</td>\n",
       "      <td>United States</td>\n",
       "      <td>Anchorage</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>61.1508</td>\n",
       "      <td>-149.1091</td>\n",
       "      <td>1_city_region_country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>florence-or-united-states</td>\n",
       "      <td>United States</td>\n",
       "      <td>Florence</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>43.9916</td>\n",
       "      <td>-124.1063</td>\n",
       "      <td>1_city_region_country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>shirley-ny-united-states</td>\n",
       "      <td>United States</td>\n",
       "      <td>Shirley</td>\n",
       "      <td>New York</td>\n",
       "      <td>40.7936</td>\n",
       "      <td>-72.8748</td>\n",
       "      <td>1_city_region_country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>juneau-ak-united-states</td>\n",
       "      <td>United States</td>\n",
       "      <td>Juneau</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>58.4546</td>\n",
       "      <td>-134.1739</td>\n",
       "      <td>1_city_region_country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lynnwood-wa-united-states</td>\n",
       "      <td>United States</td>\n",
       "      <td>Lynnwood</td>\n",
       "      <td>Washington</td>\n",
       "      <td>47.8284</td>\n",
       "      <td>-122.3033</td>\n",
       "      <td>1_city_region_country</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           id        country       city      region  latitude  \\\n",
       "0  anchorage-ak-united-states  United States  Anchorage      Alaska   61.1508   \n",
       "1   florence-or-united-states  United States   Florence      Oregon   43.9916   \n",
       "2    shirley-ny-united-states  United States    Shirley    New York   40.7936   \n",
       "3     juneau-ak-united-states  United States     Juneau      Alaska   58.4546   \n",
       "4   lynnwood-wa-united-states  United States   Lynnwood  Washington   47.8284   \n",
       "\n",
       "   longitude                  match  \n",
       "0  -149.1091  1_city_region_country  \n",
       "1  -124.1063  1_city_region_country  \n",
       "2   -72.8748  1_city_region_country  \n",
       "3  -134.1739  1_city_region_country  \n",
       "4  -122.3033  1_city_region_country  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop duplicates because a single landing point will be listed multiple times if connected to multiple `cable_id`s\n",
    "unique_location_df = location_data[['id', 'country', 'city', 'region']].drop_duplicates()\n",
    "\n",
    "# CLEANING: US State abbreviations to full name\n",
    "#     US states are set to abbreviations in submarine which is great, but lat/long datasets have full name\n",
    "def clean_us_state(df_row):\n",
    "    if df_row['country'] == 'United States':\n",
    "        return abbrev_to_us_state.get(df_row['region'])\n",
    "    return df_row['region']\n",
    "\n",
    "unique_location_df['region'] = unique_location_df.apply(lambda x:  clean_us_state(x), axis=1)\n",
    "\n",
    "\n",
    "### Do Matching\n",
    "match_1 = (\n",
    "    unique_location_df[unique_location_df['region'].notnull()]\n",
    "    .merge(\n",
    "        lat_long_city_admin_country, \n",
    "        how='inner', \n",
    "        on=['country', 'city', 'region'])\n",
    ")\n",
    "match_1['match'] = \"1_city_region_country\"\n",
    "remaining = unique_location_df[~unique_location_df['id'].isin(match_1['id'])]\n",
    "print(f\"\\nmatch 1 percentage: {match_1.shape[0] / unique_location_df.shape[0]:.4f}, count: {match_1.shape[0]}\")\n",
    "print(f\"remaining percentage: {remaining.shape[0]/ unique_location_df.shape[0]:.4f}, count: {remaining.shape[0]}\")\n",
    "\n",
    "\n",
    "match_2 = (\n",
    "    remaining\n",
    "    .merge(\n",
    "        lat_long_city_country, \n",
    "        how='inner', \n",
    "        on=['country', 'city'])\n",
    ")\n",
    "match_2['match'] = \"2_city_country\"\n",
    "remaining = remaining[~remaining['id'].isin(match_2['id'])]\n",
    "print(f\"\\nmatch 2 percentage: {match_2.shape[0] / unique_location_df.shape[0]:.4f}, count: {match_2.shape[0]}\")\n",
    "print(f\"remaining percentage: {remaining.shape[0]/ unique_location_df.shape[0]:.4f}, count: {remaining.shape[0]}\")\n",
    "\n",
    "\n",
    "match_3 = (\n",
    "    remaining\n",
    "    .merge(lat_long_usa_state, how='inner', on=['country', 'region'])\n",
    ")\n",
    "match_3['match'] = \"3_usa_state\"\n",
    "remaining = remaining[~remaining['id'].isin(match_3['id'])]\n",
    "print(f\"\\nmatch 3 percentage: {match_3.shape[0] / unique_location_df.shape[0]:.4f}, count: {match_3.shape[0]}\")\n",
    "print(f\"remaining percentage: {remaining.shape[0]/ unique_location_df.shape[0]:.4f}, count: {remaining.shape[0]}\")\n",
    "\n",
    "\n",
    "match_4 = (\n",
    "    remaining\n",
    "    .merge(lat_long_country, how='inner', on=['country'])\n",
    ")\n",
    "match_4['match'] = \"4_country\"\n",
    "remaining = remaining[~remaining['id'].isin(match_4['id'])]\n",
    "print(f\"\\nmatch 4 percentage: {match_4.shape[0] / unique_location_df.shape[0]:.4f}, count: {match_4.shape[0]}\")\n",
    "print(f\"remaining percentage: {remaining.shape[0]/ unique_location_df.shape[0]:.4f}, count: {remaining.shape[0]}\")\n",
    "\n",
    "\n",
    "match_5 = (\n",
    "    remaining\n",
    "    .merge(lat_long_country_hardcoded, how='inner', on=['country'])\n",
    ")\n",
    "match_5['match'] = \"5_country_hardcoded\"\n",
    "remaining = remaining[~remaining['id'].isin(match_5['id'])]\n",
    "print(f\"\\nmatch 4 percentage: {match_5.shape[0] / unique_location_df.shape[0]:.4f}, count: {match_5.shape[0]}\")\n",
    "print(f\"remaining percentage: {remaining.shape[0]/ unique_location_df.shape[0]:.4f}, count: {remaining.shape[0]}\")\n",
    "\n",
    "location_with_lat_long = pd.concat(\n",
    "    [match_1, match_2, match_3, match_4, match_5],\n",
    "    axis=0\n",
    ")\n",
    "assert location_with_lat_long.shape[0] == unique_location_df.shape[0]\n",
    "\n",
    "location_with_lat_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae2f5522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add country granularity as well for a consistent view\n",
    "all_country_lat_long = (\n",
    "    pd.concat([lat_long_country_hardcoded, lat_long_country], axis=0)\n",
    "    .rename(columns={'latitude': 'latitide_country', 'longitude': 'longitude_country'})\n",
    ")\n",
    "location_with_lat_long_country = (\n",
    "    location_with_lat_long\n",
    "    .merge(all_country_lat_long, on='country', how='inner')\n",
    ")\n",
    "assert location_with_lat_long_country.shape[0] == location_with_lat_long.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d72fb7a",
   "metadata": {},
   "source": [
    "# Write Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "87789de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = Path(\"cleaned_data\")\n",
    "folder.mkdir(exist_ok=True)\n",
    "\n",
    "cable_data.to_csv(folder / \"cable.csv\")\n",
    "owner_data.to_csv(folder / \"owner.csv\")\n",
    "location_data.to_csv(folder / \"location.csv\")\n",
    "location_with_lat_long_country.to_csv(folder / \"location_lat_long.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
